{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0W-WuYsjWPP"
   },
   "source": [
    "# Artificial Intelligence Course - Fall 1402\n",
    "## Computer Assignment #2 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksnYjMyNPAcn"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Part 1: Value Iteration & Policy Iteration Algorithms](#1)\n",
    "    - [ŸéQuestion 1:](#1-0)\n",
    "    - [ŸéQuestion 2:](#1-1)\n",
    "    - [ŸéQuestion 3:](#1-12)\n",
    "    - [ŸéQuestion 4:](#1-2)\n",
    "    - [ŸéQuestion 5:](#1-3)\n",
    "        - [Value Iteration](#1-3-1)\n",
    "        - [Policy Iteration](#1-3-2)\n",
    "    - [ŸéQuestion 6:](#1-4)\n",
    "        - [Value Iteration](#1-4-1)\n",
    "        - [Policy Iteration](#1-4-2)\n",
    "- [Part 2: Q-Learning Algorithm](#2)\n",
    "    - [ŸéQuestion 8:](#2-1)\n",
    "    - [ŸéQuestion 9:](#2-2)\n",
    "    - [ŸéQuestion 10:](#2-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "dpTWKluXMHP5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EifP8FUKLXE7"
   },
   "source": [
    "<a name='1'></a>\n",
    "## Part 1: Value Iteration & Policy Iteration Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "8LizJeOYRMEq"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZARu6LDSqj_",
    "outputId": "2f22167b-c6ef-493d-bec3-264a93e5d8cb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you can see the environment in each step by render command :\n"
     ]
    }
   ],
   "source": [
    "# get familiar with the environment\n",
    "print(\"you can see the environment in each step by render command :\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5QTTUSrSM-L",
    "outputId": "206e612c-52ca-49d4-91b8-31f9faf5f1bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total no. of states\n",
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeaBqUeZSNNY",
    "outputId": "46e8a041-a1e4-4e43-c22a-4e2c1a3a3ddc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total no. of actions\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVJmGmCUnIGR"
   },
   "source": [
    "<a name='1-0'></a>\n",
    "### Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRDhQMwwnK2s"
   },
   "source": [
    "Value Iteration iteratively updates the state values until they converge, and then it derives the optimal policy based on these values. The algorithm is guaranteed to find the optimal policy for a finite MDP (Markov Decision Process) given a sufficiently small threshold and an appropriate discount factor. It's an efficient method for solving reinforcement learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO24LtBGLXZ7"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "VKP8LjK5jGoW"
   },
   "outputs": [],
   "source": [
    "class ValueIteration():\n",
    "    def __init__(self, env, discount_factor, theta=1e-8):\n",
    "        self.env = env\n",
    "        self.discount_factor = discount_factor\n",
    "        self.theta = theta\n",
    "        self.reset()\n",
    "        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n",
    "        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n",
    "        self.state_values[self.env.observation_space.n - 1] = 0\n",
    "        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n",
    "\n",
    "    def value_estimation(self):\n",
    "        self.delta = np.inf\n",
    "\n",
    "        while(self.delta > self.theta):\n",
    "\n",
    "            self.delta = 0\n",
    "\n",
    "            for state in range(self.env.observation_space.n):\n",
    "\n",
    "                v = self.state_values[state]\n",
    "\n",
    "                for action in range(self.env.action_space.n):\n",
    "                    action_value = 0\n",
    "                    for probability, next_state, reward, done in self.env.P[state][action]:\n",
    "                        ### START CODE HERE ###\n",
    "                        for probability, next_state, reward, done in self.env.P[state][action]:\n",
    "                            action_value += probability * (reward + self.discount_factor * self.state_values[next_state])\n",
    "                        ### END CODE HERE ###\n",
    "                    self.q_values[state, action] = action_value\n",
    "\n",
    "                self.state_values[state] = np.max(self.q_values[state,:])\n",
    "\n",
    "                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n",
    "\n",
    "                if (self.delta < self.theta):\n",
    "                    break\n",
    "\n",
    "    def take_action(self, action):\n",
    "        test_step = self.env.step(action)\n",
    "        return test_step\n",
    "        # next_state, reward, done, _ = self.env.step(action)\n",
    "        # return next_state, reward, done\n",
    "\n",
    "    def get_optimal_policy(self, state):\n",
    "        return np.argmax(self.q_values[state,:])\n",
    "\n",
    "    def get_state_values(self):\n",
    "        return self.state_values\n",
    "\n",
    "    def get_q_values(self):\n",
    "        return self.q_values\n",
    "\n",
    "    def reset(self):\n",
    "        initial_state = self.env.reset()\n",
    "        return initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frjc5mR4ncm1"
   },
   "source": [
    "<a name='1-12'></a>\n",
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMJJpf1tnddF"
   },
   "source": [
    "Policy Iteration alternates between evaluating the current policy and improving it. This process continues until the policy no longer changes, indicating that the optimal policy has been found. Policy Iteration is guaranteed to converge to the optimal policy for a finite MDP given an appropriate discount factor and a stopping criterion. It's a powerful and flexible algorithm that can handle complex reinforcement learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4DcH5yJLXqH"
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1698433256083,
     "user": {
      "displayName": "Mohammad Saadati",
      "userId": "00242434153251678664"
     },
     "user_tz": -210
    },
    "id": "XjSb1lX147hd"
   },
   "outputs": [],
   "source": [
    "class PolicyIteration():\n",
    "    def __init__(self, env, discount_factor, theta=1e-8):\n",
    "        self.env = env\n",
    "        self.discount_factor = discount_factor\n",
    "        self.theta = theta\n",
    "        self.reset()\n",
    "        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n",
    "        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n",
    "        self.state_values[self.env.observation_space.n - 1] = 0\n",
    "        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n",
    "        self.policy = np.random.randint(self.env.action_space.n, size=self.env.observation_space.n) # initial policy\n",
    "        self.policy_stable = False\n",
    "    \n",
    "    def policy_evaluation(self):\n",
    "        self.delta = np.inf\n",
    "\n",
    "        while(self.delta >= self.theta):\n",
    "\n",
    "            self.delta = 0\n",
    "\n",
    "            for state in range(self.env.observation_space.n):\n",
    "\n",
    "                v = self.state_values[state]\n",
    "\n",
    "                new_state_value = 0\n",
    "                for probability, next_state, reward, done in self.env.P[state][self.policy[state]]:\n",
    "                    ### START CODE HERE ###\n",
    "                    new_state_value += probability * (reward + self.discount_factor * self.state_values[next_state])\n",
    "                    ### END CODE HERE ###\n",
    "                self.state_values[state] = new_state_value\n",
    "\n",
    "                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        self.policy_stable = True\n",
    "\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            old_policy = self.policy[state]\n",
    "\n",
    "            for action in range(self.env.action_space.n):\n",
    "\n",
    "                action_value = 0\n",
    "                for probability, next_state, reward, done in self.env.P[state][action]:\n",
    "                    ### START CODE HERE ###\n",
    "                    action_value += probability * (reward + self.discount_factor * self.state_values[next_state])\n",
    "                    ### END CODE HERE ###\n",
    "                self.q_values[state, action] = action_value\n",
    "\n",
    "            self.policy[state] = np.argmax(self.q_values[state,:])\n",
    "\n",
    "            if old_policy != self.policy[state]:\n",
    "                self.policy_stable = False\n",
    "\n",
    "    def policy_estimation(self):\n",
    "        self.policy_stable = False\n",
    "\n",
    "        while not self.policy_stable:\n",
    "            self.policy_evaluation()\n",
    "            self.policy_improvement()\n",
    "\n",
    "    def take_action(self, action):\n",
    "        test_step = self.env.step(action)\n",
    "        return test_step\n",
    "        # next_state, reward, done = self.env.step(action)\n",
    "        # return next_state, reward, done\n",
    "\n",
    "    def get_optimal_policy(self, state):\n",
    "        return self.policy[state]\n",
    "\n",
    "    def get_state_values(self):\n",
    "        return self.state_values\n",
    "\n",
    "    def get_q_values(self):\n",
    "        return self.q_values\n",
    "\n",
    "    def reset(self):\n",
    "        initial_state = self.env.reset()\n",
    "        return initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4G-kVjmLYj4"
   },
   "source": [
    "<a name='1-3'></a>\n",
    "### Question 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PB651-ZY4vjE"
   },
   "source": [
    "<a name='1-3-1'></a>\n",
    "#### Value Iteration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>-Calculate State Action Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RGYLOfYAuKjY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Action Values:\n",
      " [[0.531441   0.59049    0.59049    0.531441  ]\n",
      " [0.531441   0.13286025 0.6561     0.59049   ]\n",
      " [0.59049    0.729      0.59049    0.6561    ]\n",
      " [0.6561     0.13286025 0.59049    0.59049   ]\n",
      " [0.59049    0.6561     0.13286025 0.531441  ]\n",
      " [0.13286025 0.13286025 0.13286025 0.13286025]\n",
      " [0.11957423 0.81       0.13286025 0.6561    ]\n",
      " [0.13286025 0.13286025 0.13286025 0.13286025]\n",
      " [0.6561     0.13286025 0.729      0.59049   ]\n",
      " [0.6561     0.81       0.81       0.11957423]\n",
      " [0.729      0.9        0.13286025 0.729     ]\n",
      " [0.13286025 0.13286025 0.13286025 0.13286025]\n",
      " [0.13286025 0.13286025 0.13286025 0.13286025]\n",
      " [0.11957423 0.81       0.9        0.729     ]\n",
      " [0.81       0.9        1.         0.81      ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vi = ValueIteration(env, discount_factor=0.9)\n",
    "vi.value_estimation()\n",
    "state_action_values = vi.get_q_values()\n",
    "print(\"State Action Values:\\n\",state_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>-Calculate State Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values:\n",
      "-------------------------------\n",
      "0.5905| 0.6561| 0.7290| 0.6561| \n",
      "-------------------------------\n",
      "0.6561| 0.1329| 0.8100| 0.1329| \n",
      "-------------------------------\n",
      "0.7290| 0.8100| 0.9000| 0.1329| \n",
      "-------------------------------\n",
      "0.1329| 0.9000| 1.0000| 0.0000| \n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "state_values = vi.get_state_values()\n",
    "line = 0\n",
    "print(\"State Values:\")\n",
    "print(\"-------------------------------\")\n",
    "for i in range(len(state_values)):\n",
    "    print(\"{:.4f}\".format(state_values[i]), end=\"| \")\n",
    "    if (i + 1) % 4 == 0 and line < 3:\n",
    "        line = line + 1\n",
    "        print(\"\\n-------------------------------\")\n",
    "print(\"\\n-------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>-Calculate Optimal Policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "----------------\n",
      "‚¨áÔ∏è| ‚û°Ô∏è| ‚¨áÔ∏è| ‚¨ÖÔ∏è| \n",
      "----------------\n",
      "‚¨áÔ∏è| ‚¨ÖÔ∏è| ‚¨áÔ∏è| ‚¨ÖÔ∏è| \n",
      "----------------\n",
      "‚û°Ô∏è| ‚¨áÔ∏è| ‚¨áÔ∏è| ‚¨ÖÔ∏è| \n",
      "----------------\n",
      "‚¨ÖÔ∏è| ‚û°Ô∏è| ‚û°Ô∏è| üéÅ|\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "line = 0\n",
    "print(\"Optimal Policy:\")\n",
    "print(\"----------------\")\n",
    "policy_chars = [\"‚¨ÖÔ∏è\", \"‚¨áÔ∏è\", \"‚û°Ô∏è\", \"‚¨ÜÔ∏è\"]\n",
    "for i in range(16):\n",
    "    if i == 15:\n",
    "        print(\"üéÅ\",end=\"|\")\n",
    "    else:\n",
    "        optimal_policy = vi.get_optimal_policy(i)\n",
    "        print(policy_chars[optimal_policy], end=\"| \")\n",
    "    if (i + 1) % 4 == 0 and line < 3:\n",
    "        line = line + 1\n",
    "        print(\"\\n----------------\")\n",
    "print(\"\\n----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>-Show Actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step0 Reward = 0\n",
      "Step1 Reward = 0\n",
      "Step2 Reward = 0\n",
      "Step3 Reward = 0\n",
      "Step4 Reward = 0\n",
      "Step5 Reward = 1\n"
     ]
    }
   ],
   "source": [
    "for j in range(5):\n",
    "    env.reset()\n",
    "    state = 0\n",
    "    for i in range(10):\n",
    "        action = vi.get_optimal_policy(state)\n",
    "        tl = vi.take_action(action)\n",
    "        next_state = tl[0]; reward = tl[1]; done = tl[2]\n",
    "        score =+ reward\n",
    "        if j == 1:\n",
    "            print(\"Step%d Reward = %d\" % (i, score))\n",
    "        # env.step(action)\n",
    "        if done == True:\n",
    "            break\n",
    "        env.render()\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipZlzoXH40Mn"
   },
   "source": [
    "<a name='1-3-2'></a>\n",
    "#### Policy Iteration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>-Calculate State Action Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Action Values:\n",
      " [[5.31441000e-01 5.90490000e-01 5.90490000e-01 5.31441000e-01]\n",
      " [5.31441000e-01 2.02082624e-08 6.56100000e-01 5.90490000e-01]\n",
      " [5.90490000e-01 7.29000000e-01 5.90490000e-01 6.56100000e-01]\n",
      " [6.56100000e-01 2.02082624e-08 5.90490000e-01 5.90490000e-01]\n",
      " [5.90490000e-01 6.56100000e-01 2.02082624e-08 5.31441000e-01]\n",
      " [2.02082624e-08 2.02082624e-08 2.02082624e-08 2.02082624e-08]\n",
      " [2.02082624e-08 8.10000000e-01 2.02082624e-08 6.56100000e-01]\n",
      " [2.02082624e-08 2.02082624e-08 2.02082624e-08 2.02082624e-08]\n",
      " [6.56100000e-01 2.02082624e-08 7.29000000e-01 5.90490000e-01]\n",
      " [6.56100000e-01 8.10000000e-01 8.10000000e-01 2.02082624e-08]\n",
      " [7.29000000e-01 9.00000000e-01 2.02082624e-08 7.29000000e-01]\n",
      " [2.02082624e-08 2.02082624e-08 2.02082624e-08 2.02082624e-08]\n",
      " [2.02082624e-08 2.02082624e-08 2.02082624e-08 2.02082624e-08]\n",
      " [2.02082624e-08 8.10000000e-01 9.00000000e-01 7.29000000e-01]\n",
      " [8.10000000e-01 9.00000000e-01 1.00000000e+00 8.10000000e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "pi = PolicyIteration(env, discount_factor=0.9)\n",
    "pi.policy_evaluation()\n",
    "pi.policy_improvement()\n",
    "pi.policy_estimation()\n",
    "q_values = pi.get_q_values()\n",
    "print(\"State Action Values:\\n\",q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>-Calculate State Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values:\n",
      "-------------------------------\n",
      "0.5905| 0.6561| 0.7290| 0.6561| \n",
      "-------------------------------\n",
      "0.6561| 0.0000| 0.8100| 0.0000| \n",
      "-------------------------------\n",
      "0.7290| 0.8100| 0.9000| 0.0000| \n",
      "-------------------------------\n",
      "0.0000| 0.9000| 1.0000| 0.0000| \n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "state_values = pi.get_state_values()\n",
    "line = 0\n",
    "print(\"State Values:\")\n",
    "print(\"-------------------------------\")\n",
    "for i in range(len(state_values)):\n",
    "    print(\"{:.4f}\".format(state_values[i]), end=\"| \")\n",
    "    if (i + 1) % 4 == 0 and line < 3:\n",
    "        line = line + 1\n",
    "        print(\"\\n-------------------------------\")\n",
    "print(\"\\n-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>-Calculate Optimal Policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "----------------\n",
      "‚¨áÔ∏è| ‚û°Ô∏è| ‚¨áÔ∏è| ‚¨ÖÔ∏è| \n",
      "----------------\n",
      "‚¨áÔ∏è| ‚¨ÖÔ∏è| ‚¨áÔ∏è| ‚¨ÖÔ∏è| \n",
      "----------------\n",
      "‚û°Ô∏è| ‚¨áÔ∏è| ‚¨áÔ∏è| ‚¨ÖÔ∏è| \n",
      "----------------\n",
      "‚¨ÖÔ∏è| ‚û°Ô∏è| ‚û°Ô∏è| üéÅ|\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "line = 0\n",
    "print(\"Optimal Policy:\")\n",
    "print(\"----------------\")\n",
    "policy_chars = [\"‚¨ÖÔ∏è\", \"‚¨áÔ∏è\", \"‚û°Ô∏è\", \"‚¨ÜÔ∏è\"]\n",
    "for i in range(16):\n",
    "    if i == 15:\n",
    "        print(\"üéÅ\",end=\"|\")\n",
    "    else:\n",
    "        optimal_policy = pi.get_optimal_policy(i)\n",
    "        print(policy_chars[optimal_policy], end=\"| \")\n",
    "    if (i + 1) % 4 == 0 and line < 3:\n",
    "        line = line + 1\n",
    "        print(\"\\n----------------\")\n",
    "print(\"\\n----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>-Show Actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step0 Reward = 0\n",
      "Step1 Reward = 0\n",
      "Step2 Reward = 0\n",
      "Step3 Reward = 0\n",
      "Step4 Reward = 0\n",
      "Step5 Reward = 1\n"
     ]
    }
   ],
   "source": [
    "for j in range(5):\n",
    "    env.reset()\n",
    "    state = 0\n",
    "    for i in range(10):\n",
    "        action = pi.get_optimal_policy(state)\n",
    "        tl = pi.take_action(action)\n",
    "        next_state = tl[0]; reward = tl[1]; done = tl[2]\n",
    "        score =+ reward\n",
    "        if j == 1:\n",
    "            print(\"Step%d Reward = %d\" % (i, score))\n",
    "        # env.step(action)\n",
    "        if done == True:\n",
    "            break\n",
    "        env.render()\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3fnAFqJLpVI"
   },
   "source": [
    "<a name='1-4'></a>\n",
    "### Question 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JuoMPH_PAcv"
   },
   "source": [
    "<a name='1-4-1'></a>\n",
    "#### Value Iteration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCBnAicAPAcv"
   },
   "source": [
    "1. **Approach**:\n",
    "\n",
    "     - Value Iteration combines policy evaluation and policy improvement into a single step. It iteratively updates the state values based on the Bellman equation for the optimal policy, and it continually improves the policy based on the updated values. Value Iteration does not explicitly maintain a policy; instead, it calculates the optimal policy indirectly through value iteration.<br><br>\n",
    "\n",
    "2. **Convergence**:\n",
    "\n",
    "     - Value Iteration usually converges faster in terms of iterations because it combines policy evaluation and improvement in a single step. However, each iteration can be computationally more expensive. Value Iteration is also guaranteed to converge to the optimal policy for a finite MDP.<br><br>\n",
    "\n",
    "3. **Computational Efficiency**:\n",
    "\n",
    "     - Value Iteration can be computationally expensive because it updates the value of all states in each iteration. It is generally faster in terms of iterations but can be slower in terms of computation.<br><br>\n",
    "\n",
    "4. **Use Cases**:\n",
    "\n",
    "   - **Value Iteration** is commonly used when you are primarily interested in finding the optimal value function and policy is a byproduct. It is suitable when the state space is large, and the focus is on convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1BMk-BfPAcv"
   },
   "source": [
    "<a name='1-4-2'></a>\n",
    "#### Policy Iteration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnUZpvLP446n"
   },
   "source": [
    "1. **Approach**:\n",
    "\n",
    "     - Policy Iteration explicitly alternates between two main steps: policy evaluation and policy improvement. It starts with an initial policy, evaluates that policy's performance, improves the policy based on the evaluated values, and repeats the process until the policy stabilizes (no longer changes). It focuses on refining the policy iteratively.<br><br>\n",
    "\n",
    "2. **Convergence**:\n",
    "\n",
    "     - Policy Iteration typically requires more iterations but fewer computations per iteration. It is guaranteed to converge to the optimal policy for a finite MDP.<br><br>\n",
    "\n",
    "3. **Computational Efficiency**:\n",
    "\n",
    "     - Policy Iteration can be computationally efficient when the policy improvement step is relatively inexpensive. It may be advantageous when you have a good initial policy.<br><br>\n",
    "\n",
    "4. **Use Cases**:\n",
    "\n",
    "   - **Policy Iteration** is often used when you want to explicitly track and improve the policy. It is a good choice when the policy space is relatively small or when you have a good initial policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLtPLm-ELpG9"
   },
   "source": [
    "<a name='2'></a>\n",
    "## Part 2: Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "3cW_rkeDMOE8"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "REPS = 20\n",
    "EPISODES = 2000\n",
    "EPSILON = 0.1\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.9\n",
    "STUDENT_NUM = 274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "L1c1w7tRMOR_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# environment\n",
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "Initial_State, _ = env.reset(seed = STUDENT_NUM)\n",
    "Initial_State, _ = env.reset(seed = STUDENT_NUM)\n",
    "Initial_State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ou0fWiX_MsZb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 1, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_row, taxi_col, pass_idx, dest_idx = env.decode(Initial_State)\n",
    "taxi_row, taxi_col, pass_idx, dest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "-8tJoWefMOdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you can see the environment in each step by render command :\n"
     ]
    }
   ],
   "source": [
    "# get familiar with the environment\n",
    "print(\"you can see the environment in each step by render command :\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "mQCB-ZIfNwJM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total no. of states\n",
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "lmk_EYbKNwYT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total no. of actions\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZJAP4nMLpiZ"
   },
   "source": [
    "<a name='2-1'></a>\n",
    "### Question 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, env, epsilon, learning_rate, discount_factor, seed):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.olr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.seed = seed\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        print(\"state: \", state)\n",
    "        state_index = state[0]  # Assuming the integer part is at index 0\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state_index, :])\n",
    "        return action\n",
    "        \n",
    "          \n",
    "            # action = np.argmax(self.q_table[state,:])\n",
    "            # action = self.get_optimal_policy(state)\n",
    "        # return action\n",
    "\n",
    "    def update_q_table(self, state, action, nextState, reward):\n",
    "        current_q_value = self.q_table[state[0]][action]\n",
    "        max_next_q_value = np.max(self.q_table[nextState])\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + \\\n",
    "                       self.learning_rate * (reward + self.discount_factor * max_next_q_value)\n",
    "        self.q_table[state[0]][action] = new_q_value\n",
    "\n",
    "    def decay_epsilon(self, episode):\n",
    "        self.epsilon = self.epsilon * 0.99\n",
    "\n",
    "    def decrease_learning_rate(self, episode):\n",
    "        self.learning_rate = self.olr / (1 + episode * 0.01)\n",
    "\n",
    "    def take_action(self, action):\n",
    "        test_step = self.env.step(action)\n",
    "        return test_step\n",
    "        # next_state, reward, done, _ = self.env.step(action)\n",
    "        # return next_state, reward, done\n",
    "\n",
    "    def get_optimal_policy(self, state):\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def get_q_values(self):\n",
    "        return self.q_table\n",
    "\n",
    "    def reset(self):\n",
    "        # self.learning_rate = self.olr\n",
    "        return self.env.reset(seed=self.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset(seed = STUDENT_NUM)\n",
    "\n",
    "for rep in range(REPS):\n",
    "    agent = QLearningAgent(env, EPSILON, LEARNING_RATE, DISCOUNT, STUDENT_NUM)\n",
    "    for episode in range(EPISODES):\n",
    "        Initial_state = env.reset(seed = STUDENT_NUM)\n",
    "\n",
    "        for step in range(EPISODES):\n",
    "            # Choose the best action\n",
    "            bestAction = agent.choose_action(Initial_state)\n",
    "\n",
    "            # Take the action and get the new state, reward, and done status\n",
    "            tl = agent.take_action(bestAction)\n",
    "            next_state = tl[0]; reward = tl[1]; done = tl[2]\n",
    "            # next_state, reward, done, info = agent.take_action(bestAction)\n",
    "\n",
    "            # Update the Q-table\n",
    "            agent.update_q_table(Initial_state, bestAction, reward, next_state)\n",
    "            Initial_state = next_state\n",
    "\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5HFAMk-Lpvs"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### Question 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code I have a function named 'decay_epsilon' that decreases the exploration parameter (epsilon) over time. This kind of decay is a common technique in reinforcement learning to shift the agent from exploration to exploitation as it gains more experience. In this case, I'm decreasing epsilon by multiplying it by 0.99 in each call to decay_epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgRhXXmwo6MT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVcMKEGDQWdU"
   },
   "source": [
    "<a name='2-3'></a>\n",
    "### Question 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlHPV0kqQWdU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
